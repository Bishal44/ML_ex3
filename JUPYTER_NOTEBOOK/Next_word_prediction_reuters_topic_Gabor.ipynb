{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "50eb1355",
      "metadata": {
        "id": "50eb1355"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from keras.layers.core import Dense, Activation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "import pickle\n",
        "from keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import heapq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import data**"
      ],
      "metadata": {
        "id": "nXwmMiUHVmN1"
      },
      "id": "nXwmMiUHVmN1"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b0f4c486",
      "metadata": {
        "id": "b0f4c486"
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "for file in os.listdir(\"sample_data/\"): # original: \"reuters_data/\"\n",
        "    if file.endswith('.sgm'): # it is important for GoogleColab\n",
        "        filename = os.path.join(\"sample_data\", file) # original: \"reuters_data\"\n",
        "        f = open(filename, 'r', encoding='utf-8', errors='ignore')\n",
        "        dataFile = f.read()\n",
        "        \n",
        "        soup = BeautifulSoup(dataFile, 'html.parser')\n",
        "        contents = soup.findAll('title')\n",
        "        \n",
        "        for content in contents:\n",
        "            documents.append(content.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of documents: {}'.format(len(documents)))"
      ],
      "metadata": {
        "id": "jwFHnhJ6YrPG",
        "outputId": "4423ebe2-b897-49c8-fb00-9a64740bbba4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "jwFHnhJ6YrPG",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 20841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2f9549e3",
      "metadata": {
        "id": "2f9549e3",
        "outputId": "b4498c45-f51b-471e-fa5a-ec012743e82d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PAXAR CORP <PAKS> MAKES ACQUISITION',\n",
              " \"<MARK'S WORK WEARHOUSE LTD> YEAR JAN 31 NET\",\n",
              " 'KEY TRONIC <KTCC> GETS NEW BUSINESS',\n",
              " 'CANADIAN BASHAW, ERSKINE RESOURCES TO MERGE',\n",
              " 'ENTOURAGE <ENTG> HAS FIRST QUARTER LOSS',\n",
              " '<MR. JAX FASHIONS INC> YEAR FEB 28 NET',\n",
              " 'DIGITAL COMMUNICATIONS<DCAI> NAMES NEW PRESIDENT',\n",
              " 'DIGITAL <DEC> IN TERADYNE <TER> LICENSING PACT',\n",
              " 'HOME INTENSIVE <KDNY> EXTENDS DIALYSIS AT HOME',\n",
              " 'BACHE CANADA BUYS TORONTO STOCK EXCHANGE SEAT']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "documents[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Join the documents**"
      ],
      "metadata": {
        "id": "fRsdiodgY4er"
      },
      "id": "fRsdiodgY4er"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9e5894ab",
      "metadata": {
        "id": "9e5894ab"
      },
      "outputs": [],
      "source": [
        "data = \"\"\n",
        "for d in documents:\n",
        "    data += d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a3a635c4",
      "metadata": {
        "id": "a3a635c4",
        "outputId": "b2a1c008-4b74-4d78-d5a7-b07f0c971a45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data: 941529\n"
          ]
        }
      ],
      "source": [
        "print('Number of data: {}'.format(len(data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "3ffe6795",
      "metadata": {
        "id": "3ffe6795",
        "outputId": "e3030149-4bbd-4175-efa7-f597e5979bca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data: 939549\n",
            "PAXAR CORP <PAKS> MAKES ACQUISITION<MARK'S WORK WEARHOUSE LTD> YEAR JAN 31 NETKEY TRONIC <KTCC> GETS\n"
          ]
        }
      ],
      "source": [
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "print('Number of data: {}'.format(len(data)))\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "225b3c6b",
      "metadata": {
        "id": "225b3c6b",
        "outputId": "1a4ffd81-0de2-408c-f2b2-ad8f9785a23e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "139107\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7877, 8, 3599, 230, 413]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# integer encode text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded_data= tokenizer.texts_to_sequences([data])[0]\n",
        "print(len(encoded_data))\n",
        "encoded_data[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "cff5b658",
      "metadata": {
        "id": "cff5b658",
        "outputId": "4ae86fd1-e244-41f6-b763-7082bf89e656",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 31097\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31097"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# determine the vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1  # 0 is reserved for padding so that's why we added 1\n",
        "print('Vocabulary Size: %d' % vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next, we need to create sequences of words to fit the model with one word as input and one word as output.**"
      ],
      "metadata": {
        "id": "PYLR_W62b3lm"
      },
      "id": "PYLR_W62b3lm"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "858f3ef5",
      "metadata": {
        "id": "858f3ef5",
        "outputId": "0f55c4ba-7f6c-4df0-8902-7b17d38e8f16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 139101\n"
          ]
        }
      ],
      "source": [
        "# create word -> word sequences\n",
        "WORD_LENGTH = 5\n",
        "prev_words = []\n",
        "next_words = []\n",
        "for i in range(1, len(encoded_data) - WORD_LENGTH):\n",
        "    prev_words.append(encoded_data[i:i + WORD_LENGTH])\n",
        "    next_words.append(encoded_data[i + WORD_LENGTH])\n",
        "print('Total Sequences: %d' % len(prev_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running this piece shows that we have a total of 139.101 input-output pairs to train the network**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jAuVDQF9ceTs"
      },
      "id": "jAuVDQF9ceTs"
    },
    {
      "cell_type": "code",
      "source": [
        "# list(len(prev_words)[:5]) # [input, output]"
      ],
      "metadata": {
        "id": "MNb-LdgdcqSZ"
      },
      "id": "MNb-LdgdcqSZ",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**We can then split the sequences into input (X) and output elements (y)**\n",
        "\n"
      ],
      "metadata": {
        "id": "kMClupvlcdWq"
      },
      "id": "kMClupvlcdWq"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f0fb1be0",
      "metadata": {
        "id": "f0fb1be0"
      },
      "outputs": [],
      "source": [
        "# split into X and y elements\n",
        "X = prev_words\n",
        "Y = next_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "6f323b68",
      "metadata": {
        "id": "6f323b68",
        "outputId": "cfd8df34-12f3-4eba-c1c4-8f19e6037ef7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[8, 3599, 230, 413, 7878], [3599, 230, 413, 7878, 851], [230, 413, 7878, 851, 4923], [413, 7878, 851, 4923, 66], [7878, 851, 4923, 66, 24]]\n",
            "[851, 4923, 66, 24, 49]\n"
          ]
        }
      ],
      "source": [
        "print(X[:5])\n",
        "print(Y[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "101d961d",
      "metadata": {
        "id": "101d961d",
        "outputId": "7f798022-76b5-448b-aafd-4050b038dbf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# one hot encode outputs\n",
        "Y = to_categorical(Y, num_classes=vocab_size)\n",
        "# define model\n",
        "Y[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the model**"
      ],
      "metadata": {
        "id": "9-SgDZVKd_IL"
      },
      "id": "9-SgDZVKd_IL"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "b459da0b",
      "metadata": {
        "id": "b459da0b",
        "outputId": "02aa7ec9-0f81-4d3f-edfb-7a57728e82d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1, 10)             310970    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50)                12200     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 31097)             1585947   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,909,117\n",
            "Trainable params: 1,909,117\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1)) # original: 5\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model**"
      ],
      "metadata": {
        "id": "F1eNfS9cebdM"
      },
      "id": "F1eNfS9cebdM"
    },
    {
      "cell_type": "code",
      "source": [
        "# fit network\n",
        "optimizer = RMSprop(lr=0.01)\n",
        "\n",
        "# compile network\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) # optimizer ='adam'\n",
        "history = model.fit(X, Y, validation_split=0.05, batch_size=50, epochs=20, shuffle=True).history\n",
        "# model.fit(X, Y, epochs=100) # alternative version"
      ],
      "metadata": {
        "id": "Pjty_axoePeu"
      },
      "id": "Pjty_axoePeu",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}