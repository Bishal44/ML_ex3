{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "50eb1355",
      "metadata": {
        "id": "50eb1355"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras.layers.core import Dense, Activation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "import pickle\n",
        "from keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import heapq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import data**"
      ],
      "metadata": {
        "id": "nXwmMiUHVmN1"
      },
      "id": "nXwmMiUHVmN1"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b0f4c486",
      "metadata": {
        "id": "b0f4c486"
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "for file in os.listdir(\"sample_data/\"): # original: \"reuters_data/\"\n",
        "    if file.endswith('.sgm'): # it is important for GoogleColab\n",
        "        filename = os.path.join(\"sample_data\", file) # original: \"reuters_data\"\n",
        "        f = open(filename, 'r', encoding='utf-8', errors='ignore')\n",
        "        dataFile = f.read()\n",
        "        \n",
        "        soup = BeautifulSoup(dataFile, 'html.parser')\n",
        "        contents = soup.findAll('title')\n",
        "        \n",
        "        for content in contents:\n",
        "            documents.append(content.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of documents: {}'.format(len(documents)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwFHnhJ6YrPG",
        "outputId": "9454261c-6f58-4fd9-d70c-ce1d52645a6a"
      },
      "id": "jwFHnhJ6YrPG",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 20841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "2f9549e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f9549e3",
        "outputId": "28d1a5dd-b33e-46b6-b9aa-b8e093e2fb76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['GANTOS INC <GTOS> 4TH QTR JAN 31 NET',\n",
              " 'CHEMLAWN CORP, ECHOLAB INC SIGN DEFINITIVE MERGER AGREEMENT\\n',\n",
              " 'LDC FOOD AID NEEDS DECLINE IN 1986/87 - USDA',\n",
              " 'U.S. SUGAR PROGRAM CUT SENT TO CONGRESS BY USDA',\n",
              " '<OE INC> 4TH QTR NET',\n",
              " 'TEXAS INSTRUMENTS <TXN> BEGINS BUILDING PLANT',\n",
              " 'VMS MORTGAGE LP <VMLPZ> MONTHLY CASH PAYOUT',\n",
              " 'LITTLE EFFECT SEEN FROM COLD STORAGE REPORT',\n",
              " 'STRIKE THREAT, LOWER TRAFFIC MAR SEAWAY OPENING',\n",
              " 'CHEMLAWN <CHEM>, ECOLAB <ECON> IN MERGER PACT']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "documents[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Join the documents**"
      ],
      "metadata": {
        "id": "fRsdiodgY4er"
      },
      "id": "fRsdiodgY4er"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9e5894ab",
      "metadata": {
        "id": "9e5894ab"
      },
      "outputs": [],
      "source": [
        "data = \"\"\n",
        "for d in documents:\n",
        "    data += d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a3a635c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3a635c4",
        "outputId": "5b7b859d-bf80-43e0-97c1-9ac9adbec469"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data: 941529\n"
          ]
        }
      ],
      "source": [
        "print('Number of data: {}'.format(len(data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3ffe6795",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ffe6795",
        "outputId": "b1414335-f092-40e7-8e7f-1c3fb4a99fec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data: 939549\n",
            "GANTOS INC <GTOS> 4TH QTR JAN 31 NETCHEMLAWN CORP, ECHOLAB INC SIGN DEFINITIVE MERGER AGREEMENTLDC F\n"
          ]
        }
      ],
      "source": [
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "print('Number of data: {}'.format(len(data)))\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "225b3c6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "225b3c6b",
        "outputId": "758940f8-787a-4560-cf9f-fb7bc03e8a37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "139107\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7877, 5, 4926, 15, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# integer encode text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded_data = tokenizer.texts_to_sequences([data])[0]\n",
        "print(len(encoded_data))\n",
        "encoded_data[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cff5b658",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cff5b658",
        "outputId": "74bbe6a3-a5b0-4688-b6f8-5427d483c493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 31096\n"
          ]
        }
      ],
      "source": [
        "# determine the vocabulary size\n",
        "unique_words = tokenizer.word_index\n",
        "# unique_words = np.unique(words) # alternative version\n",
        "vocab_size = len(unique_words) + 1  # 0 is reserved for padding so that's why we added 1\n",
        "unique_word_index = dict((c, i) for i, c in enumerate(unique_words))\n",
        "print('Vocabulary Size: %d' % vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "unique_word_index = dict((c, i) for i, c in enumerate(unique_words))"
      ],
      "metadata": {
        "id": "_dPJH9Brwh9L"
      },
      "id": "_dPJH9Brwh9L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next, we need to create sequences of words to fit the model with one word as input and one word as output.**"
      ],
      "metadata": {
        "id": "PYLR_W62b3lm"
      },
      "id": "PYLR_W62b3lm"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "858f3ef5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "858f3ef5",
        "outputId": "e71452a6-1458-4eb9-e2bb-efe999a34e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 139101\n"
          ]
        }
      ],
      "source": [
        "# create word -> word sequences\n",
        "WORD_LENGTH = 5\n",
        "prev_words = []\n",
        "next_words = []\n",
        "for i in range(1, len(encoded_data) - WORD_LENGTH):\n",
        "    prev_words.append(encoded_data[i:i + WORD_LENGTH])\n",
        "    next_words.append(encoded_data[i + WORD_LENGTH])\n",
        "print('Total Sequences: %d' % len(prev_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running this piece shows that we have a total of 139.101 input-output pairs to train the network**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jAuVDQF9ceTs"
      },
      "id": "jAuVDQF9ceTs"
    },
    {
      "cell_type": "code",
      "source": [
        "# list(len(prev_words)[:5]) # [input, output]"
      ],
      "metadata": {
        "id": "MNb-LdgdcqSZ"
      },
      "id": "MNb-LdgdcqSZ",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**We can then split the sequences into input (X) and output elements (y)**\n",
        "\n"
      ],
      "metadata": {
        "id": "kMClupvlcdWq"
      },
      "id": "kMClupvlcdWq"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f0fb1be0",
      "metadata": {
        "id": "f0fb1be0"
      },
      "outputs": [],
      "source": [
        "# split into X and y elements\n",
        "X = prev_words\n",
        "X = np.array(X)\n",
        "Y = next_words\n",
        "Y = np.array(Y)\n",
        "\n",
        "# X = np.zeros((len(prev_words), WORD_LENGTH, vocab_size), dtype=bool)\n",
        "# Y = np.zeros((len(next_words), vocab_size), dtype=bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f323b68",
      "metadata": {
        "id": "6f323b68"
      },
      "outputs": [],
      "source": [
        "print(X[:5])\n",
        "print(Y[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "101d961d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "101d961d",
        "outputId": "aa8113bb-b22b-4f2a-d706-7adce18a832a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# one hot encode outputs\n",
        "Y = to_categorical(Y, num_classes=vocab_size)\n",
        "# define model\n",
        "Y[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the model**"
      ],
      "metadata": {
        "id": "9-SgDZVKd_IL"
      },
      "id": "9-SgDZVKd_IL"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b459da0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b459da0b",
        "outputId": "e129176b-56a8-4344-a67a-4db2649e3fb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1, 10)             310960    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 50)                12200     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 31096)             1585896   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,909,056\n",
            "Trainable params: 1,909,056\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1)) # original: 5\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model**"
      ],
      "metadata": {
        "id": "F1eNfS9cebdM"
      },
      "id": "F1eNfS9cebdM"
    },
    {
      "cell_type": "code",
      "source": [
        "# fit network\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "\n",
        "# compile network\n",
        "#### since labels are INTEGERS, we need to changed from loss='categorical_crossentropy'!!!\n",
        "#### If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss.\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) # optimizer ='adam'\n",
        "history = model.fit(X, Y, batch_size=50, epochs=20, shuffle=True).history\n",
        "\n",
        "## Alternative versions\n",
        "# history = model.fit(X, Y, validation_split=0.05, batch_size=50, epochs=20, shuffle=True).history\n",
        "# model.fit(X, Y, epochs=100)\n",
        "# model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pjty_axoePeu",
        "outputId": "628e4eee-506f-4808-93f1-41ce54093548"
      },
      "id": "Pjty_axoePeu",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 5).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 5).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2783/2783 [==============================] - 28s 8ms/step - loss: 8.6571 - accuracy: 0.0766\n",
            "Epoch 2/20\n",
            "2783/2783 [==============================] - 25s 9ms/step - loss: 9.1894 - accuracy: 0.1039\n",
            "Epoch 3/20\n",
            "2783/2783 [==============================] - 25s 9ms/step - loss: 9.0665 - accuracy: 0.1177\n",
            "Epoch 4/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.9297 - accuracy: 0.1300\n",
            "Epoch 5/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.8020 - accuracy: 0.1403\n",
            "Epoch 6/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.7033 - accuracy: 0.1493\n",
            "Epoch 7/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.6212 - accuracy: 0.1571\n",
            "Epoch 8/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.5570 - accuracy: 0.1635\n",
            "Epoch 9/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.5062 - accuracy: 0.1686\n",
            "Epoch 10/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.4646 - accuracy: 0.1736\n",
            "Epoch 11/20\n",
            "2783/2783 [==============================] - 25s 9ms/step - loss: 8.4316 - accuracy: 0.1772\n",
            "Epoch 12/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.4005 - accuracy: 0.1813\n",
            "Epoch 13/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.3714 - accuracy: 0.1847\n",
            "Epoch 14/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.3468 - accuracy: 0.1867\n",
            "Epoch 15/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.3301 - accuracy: 0.1889\n",
            "Epoch 16/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.3124 - accuracy: 0.1914\n",
            "Epoch 17/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.2973 - accuracy: 0.1951\n",
            "Epoch 18/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.2830 - accuracy: 0.1956\n",
            "Epoch 19/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.2807 - accuracy: 0.1971\n",
            "Epoch 20/20\n",
            "2783/2783 [==============================] - 22s 8ms/step - loss: 8.2637 - accuracy: 0.1986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save trained model**"
      ],
      "metadata": {
        "id": "QxgsBCrwtECi"
      },
      "id": "QxgsBCrwtECi"
    },
    {
      "cell_type": "code",
      "source": [
        "# After successful training, we will save the trained model and just load it back as needed.\n",
        "model.save('keras_next_word_model.h5')\n",
        "pickle.dump(history, open(\"history.p\", \"wb\"))\n",
        "\n",
        "model = load_model('keras_next_word_model.h5')\n",
        "history = pickle.load(open(\"history.p\", \"rb\"))"
      ],
      "metadata": {
        "id": "A2ioJcaGtEN4"
      },
      "id": "A2ioJcaGtEN4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction**\n",
        "Using saved model:\n",
        "- we input the sample as a feature vector\n",
        "- we convert the input string to a single feature vector"
      ],
      "metadata": {
        "id": "XsrX16ketfZq"
      },
      "id": "XsrX16ketfZq"
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_input(text):\n",
        "    x = np.zeros((1, WORD_LENGTH, vocab_size))\n",
        "    for t, word in enumerate(text.split()):\n",
        "        print(word)\n",
        "        x[0, t, unique_word_index[word]] = 1\n",
        "    return x\n",
        "prepare_input(\"HOSPITAL CORP SAYS IT RECEIVED 47 DLR A SHARE OFFER FROM INVESTOR GROUP\".lower())"
      ],
      "metadata": {
        "id": "4UPDwpgBtfkF"
      },
      "id": "4UPDwpgBtfkF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To choose the best possible \"n\" words after the prediction from the model ...\n",
        "def sample(preds, top_n=3):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds)\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    return heapq.nlargest(top_n, range(len(preds)), preds.take)"
      ],
      "metadata": {
        "id": "fmfAeTHUt5aL"
      },
      "id": "fmfAeTHUt5aL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the function predict_completions to predict and return the list of \"n\" predicted words.\n",
        "def predict_completions(text, n=3):\n",
        "    if text == \"\":\n",
        "        return(\"0\")\n",
        "    x = prepare_input(text)\n",
        "    preds = model.predict(x, verbose=0)[0]\n",
        "    next_indices = sample(preds, n)\n",
        "    return [unique_words[idx] for idx in next_indices]"
      ],
      "metadata": {
        "id": "r75h3atZuDxS"
      },
      "id": "r75h3atZuDxS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use tokenizer.tokenize fo removing the punctuations and also we choose 5 first words because our predicts base on 5 previous words.\n",
        "q =  \"GILLETTE CANADA ISSUES 70 MLN STG BOND\"\n",
        "print(\"correct sentence: \",q)\n",
        "seq = \" \".join(tokenizer.tokenize(q.lower())[0:5])\n",
        "print(\"Sequence: \",seq)\n",
        "print(\"next possible words: \", predict_completions(seq, 5))"
      ],
      "metadata": {
        "id": "Cv5GjUyDuaNp"
      },
      "id": "Cv5GjUyDuaNp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Prediction script**"
      ],
      "metadata": {
        "id": "H41rOg1XpXUl"
      },
      "id": "H41rOg1XpXUl"
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the model and tokenizer\n",
        "\n",
        "model = history\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "    \"\"\"\n",
        "        In this function we are using the tokenizer and models trained\n",
        "        and we are creating the sequence of the text entered and then\n",
        "        using our model to predict and return the the predicted word.\n",
        "    \n",
        "    \"\"\"\n",
        "    for i in range(3):\n",
        "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "        sequence = np.array(sequence)\n",
        "        \n",
        "        preds = model.predict_classes(sequence)\n",
        "#         print(preds)\n",
        "        predicted_word = \"\"\n",
        "        \n",
        "        for key, value in tokenizer.word_index.items():\n",
        "            if value == preds:\n",
        "                predicted_word = key\n",
        "                break\n",
        "        \n",
        "        print(predicted_word)\n",
        "        return predicted_word"
      ],
      "metadata": {
        "id": "jVlQUV23pa6X"
      },
      "id": "jVlQUV23pa6X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    We are testing our model and we will run the model\n",
        "    until the user decides to stop the script.\n",
        "    While the script is running we try and check if \n",
        "    the prediction can be made on the text. If no\n",
        "    prediction can be made we just continue.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# text1 = \"at the dull\"\n",
        "# text2 = \"collection of textile\"\n",
        "# text3 = \"what a strenuous\"\n",
        "# text4 = \"stop the script\"\n",
        "\n",
        "while(True):\n",
        "\n",
        "    text = input(\"Enter your line: \")\n",
        "    \n",
        "    if text == \"stop the script\":\n",
        "        print(\"Ending The Program.....\")\n",
        "        break\n",
        "    \n",
        "    else:\n",
        "        try:\n",
        "            text = text.split(\" \")\n",
        "            text = text[-1]\n",
        "\n",
        "            text = ''.join(text)\n",
        "            Predict_Next_Words(model, tokenizer, text)\n",
        "            \n",
        "        except:\n",
        "            continue"
      ],
      "metadata": {
        "id": "LxZ-ZlCkpgZ0",
        "outputId": "0c39010f-4520-4860-a51a-3a940d7c23d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "LxZ-ZlCkpgZ0",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your line: ilona\n",
            "Enter your line: ISRAELI HELICOPTERS\n",
            "Enter your line: GILLETTE CANADA ISSUES\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}