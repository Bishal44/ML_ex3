{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "50eb1355",
      "metadata": {
        "id": "50eb1355"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import string\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.models import load_model\n",
        "from keras.layers.core import Dense, Activation\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "import pickle\n",
        "from keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import heapq\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import data**"
      ],
      "metadata": {
        "id": "nXwmMiUHVmN1"
      },
      "id": "nXwmMiUHVmN1"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b0f4c486",
      "metadata": {
        "id": "b0f4c486"
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "for file in os.listdir(\"sample_data/\"): # original: \"reuters_data/\"\n",
        "    if file.endswith('.sgm'): # it is important for GoogleColab\n",
        "        filename = os.path.join(\"sample_data\", file) # original: \"reuters_data\"\n",
        "        f = open(filename, 'r', encoding='utf-8', errors='ignore')\n",
        "        dataFile = f.read()\n",
        "        \n",
        "        soup = BeautifulSoup(dataFile, 'html.parser')\n",
        "        contents = soup.findAll('title')\n",
        "        \n",
        "        for content in contents:\n",
        "            documents.append(content.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of documents: {}'.format(len(documents)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwFHnhJ6YrPG",
        "outputId": "4852796c-e5a8-4154-e3ec-e0580e920b00"
      },
      "id": "jwFHnhJ6YrPG",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 20841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2f9549e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f9549e3",
        "outputId": "7abb0bf0-221c-4c03-df3b-290ede74cd12"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['GANTOS INC <GTOS> 4TH QTR JAN 31 NET',\n",
              " 'CHEMLAWN CORP, ECHOLAB INC SIGN DEFINITIVE MERGER AGREEMENT\\n',\n",
              " 'LDC FOOD AID NEEDS DECLINE IN 1986/87 - USDA',\n",
              " 'U.S. SUGAR PROGRAM CUT SENT TO CONGRESS BY USDA',\n",
              " '<OE INC> 4TH QTR NET',\n",
              " 'TEXAS INSTRUMENTS <TXN> BEGINS BUILDING PLANT',\n",
              " 'VMS MORTGAGE LP <VMLPZ> MONTHLY CASH PAYOUT',\n",
              " 'LITTLE EFFECT SEEN FROM COLD STORAGE REPORT',\n",
              " 'STRIKE THREAT, LOWER TRAFFIC MAR SEAWAY OPENING',\n",
              " 'CHEMLAWN <CHEM>, ECOLAB <ECON> IN MERGER PACT']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "documents[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Join the documents**"
      ],
      "metadata": {
        "id": "fRsdiodgY4er"
      },
      "id": "fRsdiodgY4er"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9e5894ab",
      "metadata": {
        "id": "9e5894ab"
      },
      "outputs": [],
      "source": [
        "data = \"\"\n",
        "for d in documents:\n",
        "    data += d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a3a635c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3a635c4",
        "outputId": "3c73f840-3fd7-49b6-cb05-4c9dcf1b3204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data: 941529\n"
          ]
        }
      ],
      "source": [
        "print('Number of data: {}'.format(len(data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3ffe6795",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ffe6795",
        "outputId": "baa472ea-41c4-4313-8f3d-1395befe533c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of data: 939549\n",
            "GANTOS INC <GTOS> 4TH QTR JAN 31 NETCHEMLAWN CORP, ECHOLAB INC SIGN DEFINITIVE MERGER AGREEMENTLDC F\n"
          ]
        }
      ],
      "source": [
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
        "print('Number of data: {}'.format(len(data)))\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "225b3c6b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "225b3c6b",
        "outputId": "0fb783a7-e988-43e6-b084-20b030ef3e7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "139107\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7877, 5, 4926, 15, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# integer encode text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "encoded_data = tokenizer.texts_to_sequences([data])[0]\n",
        "print(len(encoded_data))\n",
        "encoded_data[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cff5b658",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cff5b658",
        "outputId": "c58cec05-2780-4583-bff8-05c92da57574"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 31096\n"
          ]
        }
      ],
      "source": [
        "# determine the vocabulary size\n",
        "unique_words = tokenizer.word_index\n",
        "# unique_words = np.unique(words) # alternative version\n",
        "vocab_size = len(unique_words) + 1  # 0 is reserved for padding so that's why we added 1\n",
        "unique_word_index = dict((c, i) for i, c in enumerate(unique_words))\n",
        "print('Vocabulary Size: %d' % vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next, we need to create sequences of words to fit the model with one word as input and one word as output.**"
      ],
      "metadata": {
        "id": "PYLR_W62b3lm"
      },
      "id": "PYLR_W62b3lm"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "858f3ef5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "858f3ef5",
        "outputId": "c4ccbb02-2a91-4677-bd19-5e7a8072de15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sequences: 139101\n"
          ]
        }
      ],
      "source": [
        "# create word -> word sequences\n",
        "WORD_LENGTH = 5\n",
        "prev_words = []\n",
        "next_words = []\n",
        "for i in range(1, len(encoded_data) - WORD_LENGTH):\n",
        "    prev_words.append(encoded_data[i:i + WORD_LENGTH])\n",
        "    next_words.append(encoded_data[i + WORD_LENGTH])\n",
        "print('Total Sequences: %d' % len(prev_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Running this piece shows that we have a total of 139.101 input-output pairs to train the network**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jAuVDQF9ceTs"
      },
      "id": "jAuVDQF9ceTs"
    },
    {
      "cell_type": "code",
      "source": [
        "# list(len(prev_words)[:5]) # [input, output]"
      ],
      "metadata": {
        "id": "MNb-LdgdcqSZ"
      },
      "id": "MNb-LdgdcqSZ",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**We can then split the sequences into input (X) and output elements (y)**\n",
        "\n"
      ],
      "metadata": {
        "id": "kMClupvlcdWq"
      },
      "id": "kMClupvlcdWq"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f0fb1be0",
      "metadata": {
        "id": "f0fb1be0"
      },
      "outputs": [],
      "source": [
        "# split into X and y elements\n",
        "X = prev_words\n",
        "X = np.array(X)\n",
        "Y = next_words\n",
        "Y = np.array(Y)\n",
        "\n",
        "# X = np.zeros((len(prev_words), WORD_LENGTH, vocab_size), dtype=bool)\n",
        "# Y = np.zeros((len(next_words), vocab_size), dtype=bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6f323b68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f323b68",
        "outputId": "d6d4e084-d908-4e81-aa13-23e69c072347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   5 4926   15    3   49]\n",
            " [4926   15    3   49   45]\n",
            " [  15    3   49   45 4927]\n",
            " [   3   49   45 4927    8]\n",
            " [  49   45 4927    8 7878]]\n",
            "[  45 4927    8 7878    5]\n"
          ]
        }
      ],
      "source": [
        "print(X[:5])\n",
        "print(Y[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "101d961d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "101d961d",
        "outputId": "192c67b0-0fb4-4bda-d19a-941d3a778055"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# one hot encode outputs\n",
        "Y = to_categorical(Y, num_classes=vocab_size)\n",
        "# define model\n",
        "Y[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the model**"
      ],
      "metadata": {
        "id": "9-SgDZVKd_IL"
      },
      "id": "9-SgDZVKd_IL"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b459da0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b459da0b",
        "outputId": "dc508a7f-b9bf-4492-d4d0-327018f3d7cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 1, 10)             310960    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                5504      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 31096)             1026168   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,342,632\n",
            "Trainable params: 1,342,632\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=1)) # original: 5\n",
        "model.add(LSTM(32)) # original: 50\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model**"
      ],
      "metadata": {
        "id": "F1eNfS9cebdM"
      },
      "id": "F1eNfS9cebdM"
    },
    {
      "cell_type": "code",
      "source": [
        "# fit network\n",
        "optimizer = RMSprop(learning_rate=0.01)\n",
        "\n",
        "# compile network\n",
        "#### since labels are INTEGERS, we need to changed from loss='categorical_crossentropy' to loss='sparse_categorical_crossentropy'!!!\n",
        "#### If you want to provide labels using one-hot representation, please use CategoricalCrossentropy loss.\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']) # optimizer ='adam'\n",
        "history = model.fit(X, Y, batch_size=50, epochs=20, shuffle=True).history\n",
        "\n",
        "## Alternative versions\n",
        "# history = model.fit(X, Y, validation_split=0.05, batch_size=50, epochs=20, shuffle=True).history\n",
        "# model.fit(X, Y, epochs=100)\n",
        "# model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
      ],
      "metadata": {
        "id": "Pjty_axoePeu"
      },
      "id": "Pjty_axoePeu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save trained model**"
      ],
      "metadata": {
        "id": "QxgsBCrwtECi"
      },
      "id": "QxgsBCrwtECi"
    },
    {
      "cell_type": "code",
      "source": [
        "# After successful training, we will save the trained model and just load it back as needed.\n",
        "model.save('keras_next_word_model.h5')\n",
        "pickle.dump(history, open(\"history.p\", \"wb\"))\n",
        "\n",
        "model = load_model('keras_next_word_model.h5')\n",
        "history = pickle.load(open(\"history.p\", \"rb\"))"
      ],
      "metadata": {
        "id": "A2ioJcaGtEN4"
      },
      "id": "A2ioJcaGtEN4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction**\n",
        "by using the saved model:\n",
        "- we input the sample as a feature vector\n",
        "- we convert the input string to a single feature vector"
      ],
      "metadata": {
        "id": "XsrX16ketfZq"
      },
      "id": "XsrX16ketfZq"
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_input(text):\n",
        "    x = np.zeros((1, WORD_LENGTH, vocab_size))\n",
        "    for t, word in enumerate(text.split()):\n",
        "        print(word)\n",
        "        x[0, t, unique_word_index[word]] = 1\n",
        "    return x\n",
        "prepare_input(\"HOSPITAL CORP SAYS IT RECEIVED 47 DLR A SHARE OFFER FROM INVESTOR GROUP\".lower())"
      ],
      "metadata": {
        "id": "4UPDwpgBtfkF"
      },
      "id": "4UPDwpgBtfkF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To choose the best possible \"n\" words after the prediction from the model ...\n",
        "def sample(preds, top_n=3):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds)\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    return heapq.nlargest(top_n, range(len(preds)), preds.take)"
      ],
      "metadata": {
        "id": "fmfAeTHUt5aL"
      },
      "id": "fmfAeTHUt5aL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the function predict_completions to predict and return the list of \"n\" predicted words.\n",
        "def predict_completions(text, n=3):\n",
        "    if text == \"\":\n",
        "        return(\"0\")\n",
        "    x = prepare_input(text)\n",
        "    preds = model.predict(x, verbose=0)[0]\n",
        "    next_indices = sample(preds, n)\n",
        "    return [unique_words[idx] for idx in next_indices]"
      ],
      "metadata": {
        "id": "r75h3atZuDxS"
      },
      "id": "r75h3atZuDxS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use tokenizer.tokenize fo removing the punctuations and also we choose 5 first words because our predicts base on 5 previous words.\n",
        "q =  \"GILLETTE CANADA ISSUES 70 MLN STG BOND\"\n",
        "\n",
        "## 20 EXAMPLES FOR EVALUATION:\n",
        "\"\"\"\n",
        "'AMES DEPARTMENT STORE <ADD> MARCH SALES UP'\n",
        "'ISRAELI HELICOPTERS RAID SOUTH LEBANON - RADIO'\n",
        "'GILLETTE CANADA ISSUES 70 MLN STG BOND'\n",
        "'DIGITAL COMMUNICATIONS <DCAI> SELLS SWITCHES'\n",
        "'ITALIAN TREASURY BILL OFFER MEETS MIXED DEMAND'\n",
        "'WESTLAND TO CUT A THIRD OF HELICOPTER WORKFORCE'\n",
        "'USDA DETAILS FREE GRAIN STOCKS UNDER LOAN'\n",
        "'FED SAYS U.S. DISCOUNT WINDOW BORROWINGS 361 MLN DLRS IN APRIL 8 WEEK'\n",
        "'HOSPITAL CORP SAYS IT RECEIVED 47 DLR A SHARE OFFER FROM INVESTOR GROUP'\n",
        "'FED SEEN BUYING DOLLARS FOR YEN IN OPEN MARKET'\n",
        "'DOLLAR ENDS LOWER IN LACKLUSTRE FRANKFURT'\n",
        "'HEALTH AND REHABILITATION <HRP> INITIAL PAYOUT'\n",
        "'SUPERMARKETS GENERAL <SGL> FIVE WEEK SALES'\n",
        "'SEKISUI CHEMICAL ISSUES EQUITY WARRANT EUROBOND'\n",
        "'WEST GERMAN BEET PLANTINGS DELAYED THREE WEEKS'\n",
        "'BURMAH OIL PROSPECTS REMAIN FAVOURABLE'\n",
        "'PARKER DRILLING CO <PKD> 2ND QTR FEB 28 LOSS'\n",
        "'TURKEY CALLS FOR DIALOGUE TO SOLVE DISPUTE'\n",
        "'INVESTMENT TECHNOLOGIES <IVES> IN REBATE PACT'\n",
        "'ENTOURAGE <ENTG> HAS FIRST QUARTER LOSS'\n",
        "\"\"\"\n",
        "\n",
        "print(\"correct sentence: \",q)\n",
        "seq = \" \".join(tokenizer.tokenize(q.lower())[0:5])\n",
        "print(\"Sequence: \",seq)\n",
        "print(\"next possible words: \", predict_completions(seq, 5))"
      ],
      "metadata": {
        "id": "Cv5GjUyDuaNp"
      },
      "id": "Cv5GjUyDuaNp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating a Prediction script** ALTERNATIVE"
      ],
      "metadata": {
        "id": "H41rOg1XpXUl"
      },
      "id": "H41rOg1XpXUl"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and tokenizer\n",
        "\n",
        "model = history\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "\n",
        "    \"\"\"\n",
        "In this function we are using the tokenizer and models trained and we are creating \n",
        "the sequence of the text entered and then using our model to predict and return \n",
        "the the predicted word.\n",
        "    \"\"\"\n",
        "\n",
        "    for i in range(3):\n",
        "        sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "        sequence = np.array(sequence)\n",
        "        \n",
        "        preds = model.predict_classes(sequence)\n",
        "#         print(preds)\n",
        "        predicted_word = \"\"\n",
        "        \n",
        "        for key, value in tokenizer.word_index.items():\n",
        "            if value == preds:\n",
        "                predicted_word = key\n",
        "                break\n",
        "        \n",
        "        print(predicted_word)\n",
        "        return predicted_word"
      ],
      "metadata": {
        "id": "jVlQUV23pa6X"
      },
      "id": "jVlQUV23pa6X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We are testing our model and we will run the model until the user decides to stop the script.\n",
        "While the script is running we try and check if the prediction can be made on the text. If no\n",
        "prediction can be made we just continue.\n",
        "\"\"\"\n",
        "\n",
        "# text1 = \"at the dull\"\n",
        "# text2 = \"collection of textile\"\n",
        "# text3 = \"what a strenuous\"\n",
        "# text4 = \"stop the script\"\n",
        "\n",
        "while(True):\n",
        "\n",
        "    text = input(\"Enter your line: \")\n",
        "    \n",
        "    if text == \"stop the script\":\n",
        "        print(\"Ending The Program.....\")\n",
        "        break\n",
        "    \n",
        "    else:\n",
        "        try:\n",
        "            text = text.split(\" \")\n",
        "            text = text[-1]\n",
        "\n",
        "            text = ''.join(text)\n",
        "            Predict_Next_Words(model, tokenizer, text)\n",
        "            \n",
        "        except:\n",
        "            continue"
      ],
      "metadata": {
        "id": "LxZ-ZlCkpgZ0"
      },
      "id": "LxZ-ZlCkpgZ0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}