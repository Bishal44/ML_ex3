{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fbb06f2-ff7e-46e1-b901-fd44b426e7cb",
   "metadata": {},
   "source": [
    "## Machine Learning - Exercise 4 (WS 2022)\n",
    "**Group (31):** Petkova Violeta (xxxxxxxx), Upadhyaya Bishal (xxxxxxxx), Gabor Toaso (12127079)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79c4d1-88b8-4da2-8025-c168c699e3d9",
   "metadata": {},
   "source": [
    "#### Selected topic: 3.2.3 Next-word prediction (Language Modelling) using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f410d2-84b5-4afc-a463-a19711bbd8e5",
   "metadata": {},
   "source": [
    "**Description:**\n",
    "We implemented a \"next word prediction model\", which consider predicting the next possible word (e.g.: the last word of a particular sentence)\n",
    "We used a methods of natural language processing, language modeling, and deep learning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Data source:**\n",
    "----\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**High level process:**\n",
    "- download the data from repository XXX,\n",
    "- pre-processing the data from the dataset,\n",
    "  - remove all unnecessary data,\n",
    "  - delete the starting and end of the dataset (?),\n",
    "  - save the pre-processed data as txt file (access the file using the encoding as utf-8),\n",
    "  - replace all (i) unnecessary extra new lines, (ii) the carriage return and (iii) the Unicode character,\n",
    "  - make sure we have only unique words (consider each word only once and remove additional repetitions) to avoid confusion,\n",
    "- start to analyse data downloaded from xxx repository,\n",
    "\n",
    "- tokenize the data (splitting bigger text corpus into smaller segments),\n",
    "  - Keras Tokenizer is used to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf.\n",
    "  - convert the texts to sequences (interpreting the text data into numbers),\n",
    "  - create the training dataset ('X'),\n",
    "  - define output for training data ('y') => 'y' contains all the next word predictions for each input 'X',\n",
    "  - calculate \"vocab_size\" by using the length extracted from \"tokenizer.word_index\" and then add 1 to it (\"0\" is reserved for padding and we start our cont from \"1\"),\n",
    "  - convert our predictions data 'y' to categorical data of the \"vocab_size\" => convert a class vector (integers) to the binary class matrix. This will be useful with our loss which will be categorical_crossentropy. \n",
    "  - improvements in pre-processing is still possible => to achieve a better loss and accuracy in lesser epochs,\n",
    "\n",
    "\n",
    "- **Predicting a sequential model**\n",
    "  - create an embedding layer and specify the input dimensions and output dimensions\n",
    "  - specify the input length as 1 since the prediction will be made on exactly one word and we receive a reposne for that word,\n",
    "  - add an LSTM layer (#1) to our model with 1000 units which returns the sequences as true - to pass it through another LSTM layer,\n",
    "  - for the next LSTM layer (#2), we also pass it throught another 1000 units (the return sequense is false by default),\n",
    "  - pass this through a hidden layer with 1000 node units using \"dense layer\" function with \"relu\" set as the activation,\n",
    "  - pass\n",
    "  - \n",
    "  \n",
    "  https://towardsdatascience.com/next-word-prediction-with-nlp-and-deep-learning-48b9fe0a17bf\n",
    "  ...\n",
    "  - ...\n",
    "  - ...\n",
    "\n",
    "\n",
    "For the next LSTM layer, we will also pass it through another 1000 units but we donâ€™t need to specify return sequence as it is false by default. We will pass this through a hidden layer with 1000 node units using the dense layer function with relu set as the activation. Finally, we pass it through an output layer with the specified vocab size and a softmax activation. The softmax activation ensures that we receive a bunch of probabilities for the outputs equal to the vocab size. The entire code for our model structure is as shown below. After we look at the model code, we will also look at the model summary and the model plot.\n",
    "\n",
    "\n",
    "- build a deep learning model (using LSTM),\n",
    "  - train model,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f0f08-4ba3-4a98-88a0-14f1db67cf32",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46fbd31-3571-43b4-b187-3f821023d957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b16227de-e907-4caa-969d-7ce2d94b4361",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7955ca-3ad0-416d-84d6-611698de06b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34f6dafa-c2e8-49c2-a321-459d1e83c8f1",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d73154b-2481-4ca8-aa67-76690affd70c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b645658-93e8-4b7c-912c-e8fd62681289",
   "metadata": {},
   "source": [
    "## Training the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7c3658-58f9-4ce2-bff9-45bfaf65d523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "303f010a-625a-46fc-bd99-c6f36c8d6f8b",
   "metadata": {},
   "source": [
    "## Predicting a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b61f9-8002-4ee5-b9a7-dae63b02bc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36728319-b4ae-4bd0-97ed-82f2a8ed0498",
   "metadata": {},
   "source": [
    "## Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee56751-d393-464e-83b4-3d3f3a04e41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1c982cf-6b24-4d70-96ed-4de137cd8fac",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba5a3d-6311-43bc-8e17-c89e74cb8f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7289c93-faa5-462e-b4a9-d2545ebbb12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b5c63-d213-4a8a-9676-17b5fc1d82d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
