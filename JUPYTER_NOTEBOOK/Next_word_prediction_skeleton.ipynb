{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fbb06f2-ff7e-46e1-b901-fd44b426e7cb",
   "metadata": {},
   "source": [
    "## Machine Learning - Exercise 3 (WS 2022)\n",
    "**Group (31):** Petkova Violeta (01636660), Upadhyaya Bishal (12119246), Gabor Toaso (12127079)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79c4d1-88b8-4da2-8025-c168c699e3d9",
   "metadata": {},
   "source": [
    "#### Selected topic: 3.2.3 Next-word prediction (Language Modelling) using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f410d2-84b5-4afc-a463-a19711bbd8e5",
   "metadata": {},
   "source": [
    "**Description:**\n",
    "We implemented a \"next word prediction model\", which consider predicting the next possible word (e.g.: the last word of a particular sentence)\n",
    "We used a methods of natural language processing, language modeling, and deep learning.\n",
    "\n",
    "\n",
    "**Data source:**\n",
    "----\n",
    "\n",
    "\n",
    "**High level process:**\n",
    "- download the data from repository XXX,\n",
    "- pre-processing the data from the dataset,\n",
    "  - remove all unnecessary data,\n",
    "  - delete the starting and end of the dataset (?),\n",
    "  - save the pre-processed data as txt file (access the file using the encoding as utf-8),\n",
    "  - replace all (i) unnecessary extra new lines, (ii) the carriage return and (iii) the Unicode character,\n",
    "  - make sure we have only unique words (consider each word only once and remove additional repetitions) to avoid confusion,\n",
    "- start to analyse data downloaded from xxx repository,\n",
    "\n",
    "- tokenize the data (splitting bigger text corpus into smaller segments),\n",
    "  - Keras Tokenizer is used to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf.\n",
    "  - convert the texts to sequences (interpreting the text data into numbers),\n",
    "  - create the training dataset ('X'),\n",
    "  - define output for training data ('y') => 'y' contains all the next word predictions for each input 'X',\n",
    "  - calculate \"vocab_size\" by using the length extracted from \"tokenizer.word_index\" and then add 1 to it (\"0\" is reserved for padding and we start our cont from \"1\"),\n",
    "  - convert our predictions data 'y' to categorical data of the \"vocab_size\" => convert a class vector (integers) to the binary class matrix. This will be useful with our loss which will be categorical_crossentropy. \n",
    "  - improvements in pre-processing is still possible => to achieve a better loss and accuracy in lesser epochs,\n",
    "\n",
    "\n",
    "- **Predicting a sequential model**\n",
    "  - create an embedding layer and specify the input dimensions and output dimensions\n",
    "  - specify the input length as 1 since the prediction will be made on exactly one word and we receive a reposne for that word,\n",
    "  - add an LSTM layer (#1) to our model with 1000 units which returns the sequences as true - to pass it through another LSTM layer,\n",
    "  - for the next LSTM layer (#2), we also pass it throught another 1000 units (the return sequense is false by default),\n",
    "  - pass this through a hidden layer with 1000 node units using \"dense layer\" function with \"relu\" set as the activation,\n",
    "  - pass\n",
    "  - ...\n",
    "  - ...\n",
    "\n",
    "For the next LSTM layer, we will also pass it through another 1000 units but we don’t need to specify return sequence as it is false by default. We will pass this through a hidden layer with 1000 node units using the dense layer function with relu set as the activation. Finally, we pass it through an output layer with the specified vocab size and a softmax activation. The softmax activation ensures that we receive a bunch of probabilities for the outputs equal to the vocab size. The entire code for our model structure is as shown below. After we look at the model code, we will also look at the model summary and the model plot.\n",
    "\n",
    "\n",
    "- build a deep learning model (using LSTM),\n",
    "  - train model,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f8b74c-3544-49d2-811c-d1e2b1aa2c57",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47f0514-6adc-4d49-b0cb-76f9c999b4cc",
   "metadata": {},
   "source": [
    "- https://www.ris-ai.com/predict-next-word-with-python\n",
    "- https://www.nltk.org/\n",
    "- https://towardsdatascience.com/next-word-prediction-with-nlp-and-deep-learning-48b9fe0a17bf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f0f08-4ba3-4a98-88a0-14f1db67cf32",
   "metadata": {},
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b46fbd31-3571-43b4-b187-3f821023d957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://repo.eb.lan.at/artifactory/api/pypi/pypi-repo/simple\n",
      "Requirement already satisfied: nltk in /opt/app-root/venv/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /opt/app-root/venv/lib/python3.8/site-packages (from nltk) (1.0.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/app-root/venv/lib/python3.8/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: click in /opt/app-root/venv/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /opt/app-root/venv/lib/python3.8/site-packages (from nltk) (4.54.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/opt/app-root/venv/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Looking in indexes: https://repo.eb.lan.at/artifactory/api/pypi/pypi-repo/simple\n",
      "Requirement already satisfied: keras in /opt/app-root/venv/lib/python3.8/site-packages (2.11.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/opt/app-root/venv/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk\n",
    "!pip3 install keras\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16227de-e907-4caa-969d-7ce2d94b4361",
   "metadata": {},
   "source": [
    "## Importing data (corpus length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b7955ca-3ad0-416d-84d6-611698de06b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 119164\n"
     ]
    }
   ],
   "source": [
    "text = open('metamorphosis_clean.txt').read().lower()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f6dafa-c2e8-49c2-a321-459d1e83c8f1",
   "metadata": {},
   "source": [
    "## Pre-processing (tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d73154b-2481-4ca8-aa67-76690affd70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words = tokenizer.tokenize(text)\n",
    "\n",
    "unique_words = np.unique(words)\n",
    "unique_word_index = dict((c, i) for i, c in enumerate(unique_words))\n",
    "\n",
    "# Next, for the feature engineering part, we need to have the unique sorted words list. \n",
    "# We also need a dictionary with each word form the unique_words list as key and its corresponding position as value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b768d61-35a2-4e84-9ec8-34d15938efb3",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd0d2af6-1690-4e3b-b509-99c2b36bf8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'morning', 'when', 'gregor', 'samsa']\n",
      "woke\n",
      "[False False False ... False False False]\n"
     ]
    }
   ],
   "source": [
    "WORD_LENGTH = 5 # Number of words considered in sequence\n",
    "prev_words = []\n",
    "next_words = []\n",
    "for i in range(len(words) - WORD_LENGTH):\n",
    "    prev_words.append(words[i:i + WORD_LENGTH])\n",
    "    next_words.append(words[i + WORD_LENGTH])\n",
    "print(prev_words[0])\n",
    "print(next_words[0])\n",
    "\n",
    "# Here, we create two numpy array X(for storing the features) and Y(for storing the corresponding label).\n",
    "X = np.zeros((len(prev_words), WORD_LENGTH, len(unique_words)), dtype=bool)\n",
    "Y = np.zeros((len(next_words), len(unique_words)), dtype=bool)\n",
    "\n",
    "# We iterate X and Y if the word is present then the corresponding position is made 1.\n",
    "for i, each_words in enumerate(prev_words):\n",
    "    for j, each_word in enumerate(each_words):\n",
    "        X[i, j, unique_word_index[each_word]] = 1\n",
    "    Y[i, unique_word_index[next_words[i]]] = 1\n",
    "    \n",
    "print(X[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b645658-93e8-4b7c-912c-e8fd62681289",
   "metadata": {},
   "source": [
    "## Training the data / Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc7c3658-58f9-4ce2-bff9-45bfaf65d523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "167/167 [==============================] - 10s 46ms/step - loss: 5.9646 - accuracy: 0.0807 - val_loss: 6.1029 - val_accuracy: 0.0715\n",
      "Epoch 2/20\n",
      "167/167 [==============================] - 8s 45ms/step - loss: 5.2072 - accuracy: 0.1422 - val_loss: 6.0991 - val_accuracy: 0.0786\n",
      "Epoch 3/20\n",
      "167/167 [==============================] - 7s 44ms/step - loss: 4.7784 - accuracy: 0.1754 - val_loss: 5.8362 - val_accuracy: 0.0992\n",
      "Epoch 4/20\n",
      "167/167 [==============================] - 7s 45ms/step - loss: 4.3521 - accuracy: 0.2278 - val_loss: 5.9531 - val_accuracy: 0.1090\n",
      "Epoch 5/20\n",
      "167/167 [==============================] - 7s 45ms/step - loss: 3.9106 - accuracy: 0.2919 - val_loss: 6.3359 - val_accuracy: 0.0769\n",
      "Epoch 6/20\n",
      "167/167 [==============================] - 7s 45ms/step - loss: 3.5197 - accuracy: 0.3706 - val_loss: 6.7665 - val_accuracy: 0.0894\n",
      "Epoch 7/20\n",
      "167/167 [==============================] - 7s 44ms/step - loss: 3.1505 - accuracy: 0.4465 - val_loss: 7.7365 - val_accuracy: 0.0554\n",
      "Epoch 8/20\n",
      "167/167 [==============================] - 7s 44ms/step - loss: 2.8005 - accuracy: 0.5222 - val_loss: 7.2441 - val_accuracy: 0.0813\n",
      "Epoch 9/20\n",
      "167/167 [==============================] - 7s 45ms/step - loss: 2.4615 - accuracy: 0.5939 - val_loss: 7.2105 - val_accuracy: 0.0804\n",
      "Epoch 10/20\n",
      "167/167 [==============================] - 7s 40ms/step - loss: 2.1613 - accuracy: 0.6568 - val_loss: 7.4769 - val_accuracy: 0.0715\n",
      "Epoch 11/20\n",
      "167/167 [==============================] - 7s 40ms/step - loss: 1.8971 - accuracy: 0.7076 - val_loss: 7.8935 - val_accuracy: 0.0652\n",
      "Epoch 12/20\n",
      "167/167 [==============================] - 7s 41ms/step - loss: 1.6878 - accuracy: 0.7484 - val_loss: 7.7662 - val_accuracy: 0.0733\n",
      "Epoch 13/20\n",
      "167/167 [==============================] - 7s 41ms/step - loss: 1.5290 - accuracy: 0.7805 - val_loss: 7.7765 - val_accuracy: 0.0581\n",
      "Epoch 14/20\n",
      "167/167 [==============================] - 6s 39ms/step - loss: 1.4086 - accuracy: 0.8033 - val_loss: 7.7298 - val_accuracy: 0.0554\n",
      "Epoch 15/20\n",
      "167/167 [==============================] - 7s 40ms/step - loss: 1.3311 - accuracy: 0.8209 - val_loss: 7.8470 - val_accuracy: 0.0590\n",
      "Epoch 16/20\n",
      "167/167 [==============================] - 6s 39ms/step - loss: 1.2473 - accuracy: 0.8331 - val_loss: 7.7463 - val_accuracy: 0.0590\n",
      "Epoch 17/20\n",
      "167/167 [==============================] - 7s 39ms/step - loss: 1.2074 - accuracy: 0.8423 - val_loss: 7.8298 - val_accuracy: 0.0599\n",
      "Epoch 18/20\n",
      "167/167 [==============================] - 6s 38ms/step - loss: 1.1891 - accuracy: 0.8480 - val_loss: 7.8395 - val_accuracy: 0.0572\n",
      "Epoch 19/20\n",
      "167/167 [==============================] - 6s 37ms/step - loss: 1.1708 - accuracy: 0.8529 - val_loss: 7.7638 - val_accuracy: 0.0500\n",
      "Epoch 20/20\n",
      "167/167 [==============================] - 6s 37ms/step - loss: 1.1432 - accuracy: 0.8589 - val_loss: 7.8716 - val_accuracy: 0.0545\n"
     ]
    }
   ],
   "source": [
    "# We use a single-layer LSTM model with 128 neurons, a fully connected layer, and a softmax function for activation.\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(WORD_LENGTH, len(unique_words))))\n",
    "model.add(Dense(len(unique_words)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Train - The model will be trained with 20 epochs with an RMSprop optimizer.\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "history = model.fit(X, Y, validation_split=0.05, batch_size=128, epochs=20, shuffle=True).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50ef00f1-a9aa-4282-a997-73e35a5ad15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('keras_next_word_model.h5')\n",
    "pickle.dump(history, open(\"history.p\", \"wb\"))\n",
    "\n",
    "model = load_model('keras_next_word_model.h5')\n",
    "history = pickle.load(open(\"history.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303f010a-625a-46fc-bd99-c6f36c8d6f8b",
   "metadata": {},
   "source": [
    "## Predicting a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05f4d1e4-8383-4812-a072-2f6088f384aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he\n",
      "slid\n",
      "back\n",
      "into\n",
      "his\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we need to predict new words using this model. \n",
    "# To do that we input the sample as a feature vector. \n",
    "# We convert the input string to a single feature vector.\n",
    "\n",
    "def prepare_input(text):\n",
    "    x = np.zeros((1, WORD_LENGTH, len(unique_words)))\n",
    "    for t, word in enumerate(text.split()):\n",
    "        print(word)\n",
    "        x[0, t, unique_word_index[word]] = 1\n",
    "    return x\n",
    "prepare_input(\"He slid back into his\".lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "670b61f9-8002-4ee5-b9a7-dae63b02bc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct sentence:  I'd get kicked out on the spot\n",
      "Sequence:  i d get kicked out\n",
      "i\n",
      "d\n",
      "get\n",
      "kicked\n",
      "out\n",
      "next possible words:  ['on', 'out', 'of', 'part', 'away']\n"
     ]
    }
   ],
   "source": [
    "# To choose the best possible n words after the prediction from the model is done by sample function.\n",
    "\n",
    "def sample(preds, top_n=3):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds)\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    return heapq.nlargest(top_n, range(len(preds)), preds.take)\n",
    "\n",
    "# Finally, for prediction, we use the function predict_completions which use \n",
    "# the model to predict and return the list of n predicted words.\n",
    "\n",
    "def predict_completions(text, n=3):\n",
    "    if text == \"\":\n",
    "        return(\"0\")\n",
    "    x = prepare_input(text)\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    next_indices = sample(preds, n)\n",
    "    return [unique_words[idx] for idx in next_indices]\n",
    "\n",
    "# Now let’s see how it predicts, we use tokenizer.tokenize fo removing the punctuations and \n",
    "# also we choose 5 first words because our predicts base on 5 previous words.\n",
    "\n",
    "q =  \"I'd get kicked out on the spot\"\n",
    "print(\"correct sentence: \",q)\n",
    "seq = \" \".join(tokenizer.tokenize(q.lower())[0:5])\n",
    "print(\"Sequence: \",seq)\n",
    "print(\"next possible words: \", predict_completions(seq, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c982cf-6b24-4d70-96ed-4de137cd8fac",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba5a3d-6311-43bc-8e17-c89e74cb8f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7289c93-faa5-462e-b4a9-d2545ebbb12a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59b5c63-d213-4a8a-9676-17b5fc1d82d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
